# 3.7 反復処理

## 本節の概要

本節では、**反復処理（Iteration）**の方法を学びます。反復処理とは、コレクション（リストやベクトル、データフレームなど）の各要素に対して、同じ処理を繰り返し適用することです。

データサイエンスでは、大量のデータに対して同じ計算を何度も実行することが頻繁にあります。例えば：
- シミュレーションを1000回実行して結果を集計する
- データセットの各行に統計量を計算する
- 複数のパラメータの組み合わせで実験を行う

本節では、for文を使わずに、より効率的で読みやすい方法で反復処理を実現する4つのパターンを学びます。

### 本節の学習目標

本節を学習することで、以下ができるようになります：

1. **反復処理の4つのパターンを理解する**
   - 指定した回数→1次元データ
   - 1次元データ→1次元データ
   - 1次元データ→データフレーム
   - データフレーム→データフレーム

2. **RとPythonで反復処理を実装する**
   - R: `replicate`, `map_dbl`, `map_dfr`, `pmap_dfr`
   - Python: 内包表記, `apply`メソッド

3. **実践的なデータ処理に応用する**
   - 統計量の計算
   - シミュレーション
   - データ分析パイプライン

4. **並列処理の基礎を理解する**
   - 並列化できる処理の判別
   - 並列処理の実装

---

## 3.7.0 はじめに

### 3.7.0.1 反復処理とは何か

**反復処理**とは、同じ処理を繰り返し実行することです。プログラミングでは、for文やwhile文を使って反復処理を書くことが多いですが、データサイエンスでは、より簡潔で読みやすい方法があります。

#### なぜfor文を使わないのか？

多くのプログラミング言語では、反復処理といえばfor文が基本です。しかし、RやPythonでデータサイエンスを行う場合、for文を使わない方が以下の利点があります：

1. **コードが短く読みやすい** - 処理の意図が明確
2. **エラーが少ない** - インデックスの管理が不要
3. **最適化されている** - 内部的に高速な処理
4. **並列化しやすい** - 後で並列処理に変更しやすい

例えば、リストの各要素を2倍にする処理を考えてみましょう：

**❌ for文を使った方法（冗長）**
```python
# Python
numbers = [1, 2, 3, 4, 5]
doubled = []
for n in numbers:
    doubled.append(n * 2)
print(doubled)
```

**✅ 関数型の方法（簡潔）**
```python
# Python
numbers = [1, 2, 3, 4, 5]
doubled = [n * 2 for n in numbers]
print(doubled)
```

本節では、このような「関数型プログラミング」のアプローチを学びます。

---

### 3.7.0.2 4つのパターンの全体像

本節では、反復処理を**入力と出力のデータ型**によって4つのパターンに分類します。

#### 📊 反復処理の4つのパターン

| パターン | 入力 | 出力 | Rの実現方法 | Pythonの実現方法 |
|:--------|:-----|:-----|:-----------|:----------------|
| **パターン1** | 指定した回数 | 1次元データ | `replicate(n, 式)` | `[式 for i in range(n)]` |
| **パターン2** | 1次元データ | 1次元データ | `入力 %>% map_dbl(関数)` | `入力.apply(関数)` |
| **パターン3** | 1次元データ | データフレーム | `入力 %>% map_dfr(関数)` | `入力.apply(関数)` |
| **パターン4** | データフレーム | データフレーム | `入力 %>% pmap_dfr(関数)` | `入力.apply(関数, axis=1)` |

#### 各パターンの使い分け

**パターン1: 指定した回数→1次元データ**
```
回数 (3回)  ⇒  [結果1, 結果2, 結果3]
```
- 用途: シミュレーションを何度も実行する
- 例: サイコロを100回振って結果を記録

**パターン2: 1次元データ→1次元データ**
```
[5, 10, 100]  ⇒  [f(5), f(10), f(100)]
```
- 用途: 各要素に同じ関数を適用
- 例: 複数のサンプルサイズでシミュレーション

**パターン3: 1次元データ→データフレーム**
```
[5, 10, 100]  ⇒  データフレーム（3行×複数列）
```
- 用途: 各要素から複数の統計量を計算
- 例: サンプルサイズごとに平均と標準偏差を計算

**パターン4: データフレーム→データフレーム**
```
データフレーム（入力）  ⇒  データフレーム（出力）
```
- 用途: 各行のパラメータで計算を実行
- 例: パラメータの組み合わせごとに実験

---

### 3.7.0.3 本節で使用するライブラリと準備

本節では、以下のライブラリを使用します。

#### R

```r
# データ操作とパイプ演算子
library(tidyverse)

# 並列処理（3.7.5節で使用）
# library(furrr)  # 必要になったらコメント解除
```

#### Python

```python
# 数値計算
import numpy as np

# データフレーム操作
import pandas as pd

# 並列処理（3.7.5節で使用）
# from pandarallel import pandarallel  # 必要になったらコメント解除
```

#### 環境の確認

以下のコマンドで、仮想環境が有効になっていることを確認してください：

```bash
# ターミナルで実行
$ venvc
# プロンプトに (class) と表示されることを確認
```

作業ディレクトリに移動します：

```bash
$ cd ~/work
```

---

### 3.7.0.4 GitHub Copilot活用の準備

本節では、反復処理のパターンを学びます。これらのパターンは、**GitHub Copilot**を使うことで、より効率的に習得できます。

#### GitHub Copilotとは？

GitHub Copilotは、コードを書く際にAIがサポートしてくれるツールです。VS Codeに統合されており、以下のことができます：

1. **コード補完**: コメントや関数名から、適切なコードを提案
2. **コード生成**: 説明を書くと、それに基づいたコードを生成
3. **エラー修正**: エラーメッセージから、修正方法を提案
4. **学習支援**: わからないコードについて、Copilot Chatで質問

#### Copilotのセットアップ確認

1. **VS Codeを開く**
2. 左下のステータスバーに**Copilotアイコン**（✨マーク）があることを確認
3. Python/Rファイルを開いて、コメントを書いてみる
4. 灰色の提案が表示されたら成功（Tabキーで受け入れ）

#### Copilot Chatの使い方

1. **Copilot Chatを開く**: `Ctrl+Shift+I`（Windows/Linux）
2. **質問を入力**: 日本語でOK
3. **コードを実験**: 提案されたコードをコピーして試す

#### AI協働学習の心構え

本節では、各セクションの最後に「💡 GitHub Copilot活用ガイド」を用意しています。これを使って、以下のような学習サイクルを回しましょう：

```
1. 教材を読んで概念を理解
2. サンプルプログラムを実行
3. 自分で似たようなプログラムを書いてみる
4. 困ったらCopilotに聞く
5. 生成されたコードを理解して実験
6. 練習問題で定着
```

**✅ やるべきこと**
- コメントを書いて、Copilotにコードを生成してもらう
- 生成されたコードを**必ず読んで理解する**
- わからない部分をCopilot Chatで質問する
- 少しずつ変更して実験する

**❌ 避けるべきこと**
- コードを読まずにコピー&ペーストだけ
- エラーが出ても理解せずに次のコードを生成
- Copilotに丸投げして自分で考えない
- 動けばOKと思って仕組みを理解しない

**大切なのは「AIと協働する」姿勢です。** Copilotは強力なパートナーですが、最終的にコードを理解するのはあなた自身です。Copilotを「一緒に学ぶ相手」として活用しましょう！

---

### 3.7.0.5 本節の構成

本節は以下の構成になっています：

- **3.7.1節**: パターン1（指定した回数→1次元データ）
- **3.7.2節**: パターン2（1次元データ→1次元データ）
- **3.7.3節**: パターン3（1次元データ→データフレーム）
- **3.7.4節**: パターン4（データフレーム→データフレーム）
- **3.7.5節**: 並列処理入門
- **3.7.6節**: 統合演習
- **3.7.7節**: まとめと自己チェックリスト

各節では、以下の流れで学習します：

1. **概念の説明** - なぜそのパターンが必要か
2. **基本文法** - RとPythonの両方
3. **実行可能なサンプルプログラム** - 完全に動くコード
4. **GitHub Copilot活用ガイド** - 効果的な学習方法

それでは、パターン1から順番に見ていきましょう！

---

## 3.7.1 パターン1: 指定した回数→1次元データ

### 3.7.1.1 概念: 同じ処理をN回繰り返す

**パターン1**は、同じ処理を指定した回数だけ繰り返し、その結果を1次元データ（ベクトルやリスト）にまとめるパターンです。

```
指定した回数 (3)   ⇒   1次元データ
                      [結果1, 結果2, 結果3]
```

#### どんなときに使うか？

このパターンは、**シミュレーション**や**モンテカルロ法**でよく使います。例えば：

- サイコロを100回振って、出た目を記録する
- 乱数を使った計算を1000回繰り返して、結果の分布を見る
- ランダムなデータを生成して、統計量を何度も計算する

#### 注意すべき点

単純にベクトルやリストを繰り返す方法（`rep()`や`* 3`）では、**最初に計算した結果が複製されるだけ**で、毎回新しい計算は行われません。

```python
# ❌ これは間違い（同じ値が3つ並ぶだけ）
import numpy as np
result = np.random.random()  # 一度だけ乱数を生成
[result] * 3  # 同じ値を3回繰り返す
#> [0.534, 0.534, 0.534]  # 全部同じ！
```

毎回**新しい計算**を実行するには、パターン1の方法を使う必要があります。

---

### 3.7.1.2 R: replicate関数の使い方

Rでは、`replicate()`関数を使って、式を指定した回数だけ評価します。

#### 基本構文

```r
replicate(n = 回数, expr = 式)
```

- `n`: 繰り返す回数
- `expr`: 評価する式（毎回実行される）

#### 動作の仕組み

`replicate()`は、`expr`に指定した式を`n`回評価し、その結果をベクトルにまとめます。重要なのは、**式が毎回評価される**ことです。

```r
# 乱数を3回生成（毎回異なる値）
replicate(n = 3, expr = runif(1))
#> [1] 0.2875775 0.7883051 0.4089769  # 異なる値が3つ
```

---

### 3.7.1.3 Python: リスト内包表記の基本

Pythonでは、**リスト内包表記（List Comprehension）**を使います。

#### 基本構文

```python
[式 for 変数 in range(回数)]
```

- `式`: 評価する式（毎回実行される）
- `変数`: ループ変数（通常は`i`を使う）
- `range(回数)`: 0からN-1までの数列を生成

#### 動作の仕組み

リスト内包表記は、`range(回数)`の各要素に対して`式`を評価し、結果をリストにまとめます。

```python
# 乱数を3回生成（毎回異なる値）
import numpy as np
[np.random.random() for i in range(3)]
#> [0.287, 0.788, 0.408]  # 異なる値が3つ
```

**ループ変数`i`について**: この例では`i`は使っていませんが、内包表記の構文として必要です。`i`は0, 1, 2と変化しますが、式の中で使わなくても問題ありません。

---

### 3.7.1.4 実例: 乱数の平均を繰り返し計算

具体例として、「0以上1未満の一様乱数を10個生成し、その平均を計算する」処理を3回繰り返してみます。

#### ステップ1: 関数を定義する

まず、一度の計算を関数にまとめます。

**R版**

```r
# 乱数x個の平均を求める関数
f1 <- function(x) {
  tmp <- runif(x)        # x個の乱数を生成
  mean(tmp)              # 平均を計算
}

# 動作確認
f1(10)
#> [1] 0.5776604  # 実行するたびに変わる
```

**Python版**

```python
import numpy as np

# 乱数x個の平均を求める関数
def f1(x):
    tmp = np.random.random(x)    # x個の乱数を生成
    return np.mean(tmp)          # 平均を計算

# 動作確認
f1(10)
#> 0.5427033207230424  # 実行するたびに変わる
```

#### ステップ2: 繰り返し実行する

この関数を3回実行して、結果をまとめます。

**R版**

```r
# f1(10)を3回実行
replicate(n = 3, expr = f1(10))
#> [1] 0.4672766 0.4712016 0.5579449
```

**Python版**

```python
# f1(10)を3回実行
[f1(10) for i in range(3)]
#> [0.4864425069985622, 0.4290935578857099, 0.535206509631883]
```

毎回異なる乱数が生成され、それぞれ異なる平均が計算されます。

---

### 3.7.1.5 よくある間違い: rep()や* 3を使う

前述の通り、単純な繰り返しでは**最初の計算結果が複製されるだけ**です。

**❌ 間違った方法（R）**

```r
# f1(10)を一度だけ計算し、その結果を3回繰り返す
rep(x = f1(10), times = 3)
#> [1] 0.481329 0.481329 0.481329  # 全て同じ！
```

**❌ 間違った方法（Python）**

```python
# f1(10)を一度だけ計算し、その結果を3回繰り返す
[f1(10)] * 3
#> [0.437, 0.437, 0.437]  # 全て同じ！
```

この違いは、**式がいつ評価されるか**によります：

- `replicate()`や内包表記: 式が**毎回**評価される
- `rep()`や`* 3`: 式が**最初に一度だけ**評価され、その結果が複製される

---

### 3.7.1.6 実行可能なサンプルプログラム

ここでは、完全に実行可能なプログラムを用意しました。

#### サンプル1: 基本的な使い方（Python）

**ファイル名**: `sample01_replicate.py`

```python
# 乱数の平均を繰り返し計算するサンプル
import numpy as np

# 乱数n個の平均を求める関数
def calculate_mean(n):
    random_numbers = np.random.random(n)
    return np.mean(random_numbers)

# 動作確認: 10個の乱数の平均を1回計算
print("1回だけ計算:")
print(calculate_mean(10))

# パターン1: 同じ計算を5回繰り返す
print("\n5回繰り返し:")
results = [calculate_mean(10) for i in range(5)]
print(results)

# 結果の統計量
print(f"\n平均: {np.mean(results):.4f}")
print(f"標準偏差: {np.std(results, ddof=1):.4f}")
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample01_replicate.py
# VS Codeでファイルを開いて、上記のコードをコピー&ペースト
$ python sample01_replicate.py
```

**期待される出力**:
```
1回だけ計算:
0.5234567890123456

5回繰り返し:
[0.4567, 0.5123, 0.4890, 0.5234, 0.4876]

平均: 0.4938
標準偏差: 0.0256
```
（乱数なので、実行するたびに異なる値が出ます）

---

#### サンプル2: 基本的な使い方（R）

**ファイル名**: `sample01_replicate.R`

```r
# 乱数の平均を繰り返し計算するサンプル
library(tidyverse)

# 乱数n個の平均を求める関数
calculate_mean <- function(n) {
  random_numbers <- runif(n)
  mean(random_numbers)
}

# 動作確認: 10個の乱数の平均を1回計算
cat("1回だけ計算:\n")
print(calculate_mean(10))

# パターン1: 同じ計算を5回繰り返す
cat("\n5回繰り返し:\n")
results <- replicate(n = 5, expr = calculate_mean(10))
print(results)

# 結果の統計量
cat("\n平均:", mean(results), "\n")
cat("標準偏差:", sd(results), "\n")
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample01_replicate.R
# VS Codeでファイルを開いて、上記のコードをコピー&ペースト
$ Rscript sample01_replicate.R
```

**期待される出力**:
```
1回だけ計算:
[1] 0.5234568

5回繰り返し:
[1] 0.4567 0.5123 0.4890 0.5234 0.4876

平均: 0.4938 
標準偏差: 0.02564312
```
（乱数なので、実行するたびに異なる値が出ます）

---

### 💡 GitHub Copilot活用ガイド

このセクションで学んだ「指定した回数の反復処理」は、シミュレーションやモンテカルロ法の基礎となる重要なパターンです。GitHub Copilotを使うことで、様々なバリエーションを試しながら理解を深めることができます。

---

#### 🚀 使えるプロンプト例

##### プロンプト例1: 基本的な繰り返し処理 [★☆☆]

**Copilot Chatに入力**:
```
Pythonで、サイコロを振る関数を作ってください。1から6のランダムな整数を返します。
その後、この関数を10回実行して、結果をリストにまとめてください。
内包表記を使ってください。
```

**期待される動作**:
- サイコロを模擬する関数が生成される
- 内包表記を使った10回の実行コード
- 結果の表示

**やってみよう**:
1. 生成されたコードを実行して、10個の結果を確認
2. 回数を変更してみる（例: 100回）
3. 各目が何回出たか数えるコードも追加で聞いてみる

---

##### プロンプト例2: 統計シミュレーション [★★☆]

**Copilot Chatに入力**:
```
Pythonで、正規分布からn個のランダムな値を生成し、その平均を返す関数を作ってください。
numpy.random.normalを使います。
この関数（n=30）を100回実行して、得られた100個の平均の分布を確認してください。
内包表記を使ってください。
```

**期待される動作**:
- 正規分布からサンプリングする関数
- 100回実行するコード
- 結果の平均と標準偏差の計算

**やってみよう**:
1. 生成されたコードを実行
2. サンプルサイズnを変更して結果の違いを観察
3. 繰り返し回数を増やしてみる（1000回など）

---

##### プロンプト例3: モンテカルロ法の基礎 [★★★]

**Copilot Chatに入力**:
```
Pythonで、モンテカルロ法を使って円周率πを推定するプログラムを作ってください。
1. 正方形内にランダムな点を生成
2. その点が円の内側にあるか判定
3. 内側の点の割合から円周率を推定
この試行を1000回繰り返して、推定値のリストを作ってください。
各試行では10000個の点を使います。
```

**期待される動作**:
- モンテカルロ法による円周率推定
- 1000回の試行結果
- 推定値の平均と真の値との比較

**やってみよう**:
1. 生成されたコードを実行して、推定値を確認
2. 点の数を変更して精度の変化を観察
3. 試行回数を増やして、推定の安定性を確認

---

#### 📚 Copilot活用のコツ

##### 1. コメントを先に書く
```python
# 乱数を使った計算を繰り返す
# サンプルサイズは10
# 繰り返し回数は5回
```
このようにコメントを書くと、Copilotが適切なコードを提案してくれます。

##### 2. 段階的に書く
一度に全部を書こうとせず、以下のように段階的に進めましょう：
1. まず関数を定義
2. 1回実行して動作確認
3. 繰り返し処理を追加
4. 結果の分析コードを追加

##### 3. 生成されたコードを必ず理解する
Copilotが生成したコードは、以下を確認しましょう：
- どのような関数を使っているか
- 引数の意味は何か
- なぜこの方法で実装しているか

わからない部分があれば、Copilot Chatで質問しましょう：
```
このコードの「range(5)」は何をしていますか？
```

##### 4. 実験する
生成されたコードを少しずつ変更して、動作を確認しましょう：
- 繰り返し回数を変える
- 関数の中身を変える
- 結果の表示方法を変える

---

#### ⚠️ 注意事項

**AIは完璧ではない**
- Copilotが生成したコードにエラーがあることもあります
- 必ず実行して、期待通りに動くか確認しましょう

**理解が第一**
- コードが動いても、理解していなければ意味がありません
- 各行が何をしているか、説明できるようになりましょう

**検証する習慣**
- 乱数を使うコードは、毎回結果が変わります
- 複数回実行して、妥当な結果か確認しましょう

**自分で考える**
- Copilotに頼りすぎず、まず自分で考えましょう
- 困ったときに使うツールとして活用するのが効果的です

---

#### 🎓 推奨される学習の流れ

```
1. 教材を読んで「指定回数の繰り返し」の概念を理解
2. sample01_replicate.py/Rを実行して動作を確認
3. 自分で似たような関数を書いてみる
4. 困ったらCopilotに「〜する関数を作って」と頼む
5. 生成されたコードを読んで、理解できない部分を質問
6. パラメータを変えて実験し、動作を確認
7. 次のプロンプト例にチャレンジ
```

**大切なのは「AIと協働する」姿勢です。** 丸投げではなく、一緒に学ぶパートナーとして活用しましょう！

---

## 3.7.2 パターン2: 1次元データ→1次元データ

### 3.7.2.1 概念: 各要素に関数を適用

**パターン2**は、1次元データ（ベクトルやリスト）の各要素に対して関数を適用し、結果を新しい1次元データにまとめるパターンです。

```
1次元データ (入力)       ⇒   1次元データ (出力)
[5, 10, 100]              [f(5), f(10), f(100)]
```

#### どんなときに使うか？

このパターンは、**パラメータを変えながら同じ計算を繰り返す**ときに使います。例えば：

- 複数のサンプルサイズ（5, 10, 100）でシミュレーションを実行
- 複数のファイル名のリストから、各ファイルのデータを読み込む
- 複数の日付について、その日の統計量を計算

#### パターン1との違い

- **パターン1**: 同じ引数で関数を繰り返す（例: `f(10)`を3回）
- **パターン2**: 異なる引数で関数を実行（例: `f(5)`, `f(10)`, `f(100)`）

---

### 3.7.2.2 R: map_dbl関数とパイプ演算子

Rでは、`tidyverse`パッケージの`map_dbl()`関数を使います。

#### 基本構文

```r
入力ベクトル %>% map_dbl(関数)
```

- `入力ベクトル`: 各要素に関数を適用するベクトル
- `関数`: 各要素に適用する関数
- `map_dbl()`: 結果を数値（double）のベクトルで返す

#### 動作の仕組み

`map_dbl()`は、入力ベクトルの各要素を関数に渡し、返ってきた数値をベクトルにまとめます。

```r
# 例: 各数値の平方根を計算
c(4, 9, 16) %>% map_dbl(sqrt)
#> [1] 2 3 4
```

#### mapファミリー

`map_dbl()`以外にも、返り値の型に応じた関数があります：

- `map_dbl()`: 数値（double）を返す
- `map_chr()`: 文字列（character）を返す
- `map_lgl()`: 論理値（logical）を返す
- `map()`: リストを返す

本節では主に`map_dbl()`を使います。

---

### 3.7.2.3 Python: applyメソッドと内包表記

Pythonでは、主に2つの方法があります。

#### 方法1: リスト内包表記

```python
[関数(x) for x in 入力リスト]
```

リストやタプル、rangeオブジェクトなど、反復可能なオブジェクトに使えます。

```python
# 例: 各数値の平方根を計算
import math
[math.sqrt(x) for x in [4, 9, 16]]
#> [2.0, 3.0, 4.0]
```

#### 方法2: Seriesのapplyメソッド

```python
入力Series.apply(関数)
```

pandasのSeriesに対して使います。結果もSeriesとして返されます。

```python
# 例: 各数値の平方根を計算
import pandas as pd
import numpy as np

s = pd.Series([4, 9, 16])
s.apply(np.sqrt)
#> 0    2.0
#> 1    3.0
#> 2    4.0
#> dtype: float64
```

#### どちらを使うべきか？

- **リスト内包表記**: シンプルで読みやすい、リストで十分な場合
- **apply**: pandasの機能を活かしたい、後の処理でSeriesを使う場合

本節では両方を紹介しますが、統一感のため**apply**を主に使います。

---

### 3.7.2.4 ベクトル化との違い

3.3.2節で学んだ**ベクトル化**と、パターン2の違いを理解しておきましょう。

#### ベクトル化（対応している関数）

NumPyやRの多くの関数は、ベクトル全体に一度に適用できます。

```python
# ベクトル化の例（apply不要）
import numpy as np
v = np.array([4, 9, 16])
np.sqrt(v)  # 全要素に一度に適用
#> array([2., 3., 4.])
```

```r
# ベクトル化の例（map不要）
v <- c(4, 9, 16)
sqrt(v)  # 全要素に一度に適用
#> [1] 2 3 4
```

#### パターン2が必要な場合

**ベクトル化に対応していない関数**や、**複雑な処理**の場合は、パターン2を使います。

```python
# 複雑な処理（ベクトル化できない）
def complex_calculation(x):
    # 複数のステップを含む処理
    tmp = np.random.random(x)
    return np.mean(tmp)

# パターン2が必要
v = pd.Series([5, 10, 100])
v.apply(complex_calculation)
```

**原則**: ベクトル化できるなら、そちらを使う方が速くて簡潔です。ベクトル化できない場合に、パターン2を使います。

---

### 3.7.2.5 実例: 複数のサンプルサイズでシミュレーション

前節で定義した関数`f1()`（乱数n個の平均を計算）を、複数のサンプルサイズで実行してみます。

#### 関数の再掲

**R版**

```r
f1 <- function(x) {
  tmp <- runif(x)
  mean(tmp)
}
```

**Python版**

```python
import numpy as np

def f1(x):
    tmp = np.random.random(x)
    return np.mean(tmp)
```

#### 複数のサンプルサイズで実行

サンプルサイズ5, 10, 100のそれぞれで、乱数の平均を計算します。

**R版**

```r
v <- c(5, 10, 100)
v %>% map_dbl(f1)
#> [1] 0.4857329 0.5322183 0.5084124
```

**Python版（方法1: 内包表記）**

```python
v = [5, 10, 100]
[f1(x) for x in v]
#> [0.454, 0.419, 0.552]
```

**Python版（方法2: apply）**

```python
import pandas as pd

v = pd.Series([5, 10, 100])
v.apply(f1)
#> 0    0.394206
#> 1    0.503949
#> 2    0.532698
#> dtype: float64
```

理論的には、どのサンプルサイズでも平均は約0.5になるはずです（0以上1未満の一様乱数の期待値は0.5）。サンプルサイズが大きいほど、0.5に近い値になる傾向があります。

---

### 3.7.2.6 パターン1をパターン2で実現

前節のパターン1（同じ引数で繰り返す）は、パターン2でも実現できます。

**R版**

```r
# パターン1の方法
replicate(n = 3, expr = f1(10))

# パターン2の方法（同じ値を3つ並べたベクトルを使う）
rep(x = 10, times = 3) %>% map_dbl(f1)
```

**Python版**

```python
# パターン1の方法
[f1(10) for i in range(3)]

# パターン2の方法（同じ値を3つ並べたSeriesを使う）
pd.Series([10] * 3).apply(f1)
```

どちらの方法でも同じ結果が得られますが、パターン1の方がシンプルで意図が明確です。ただし、**並列処理**を行う場合（3.7.5節）は、パターン2の方が便利なことがあります。

---

### 3.7.2.7 実行可能なサンプルプログラム

#### サンプル1: 複数のサンプルサイズで統計量を計算（Python）

**ファイル名**: `sample02_map_vector.py`

```python
# 複数のサンプルサイズでシミュレーションを実行
import numpy as np
import pandas as pd

# 乱数n個の平均を計算する関数
def calculate_mean(n):
    random_numbers = np.random.random(n)
    return np.mean(random_numbers)

# 複数のサンプルサイズを定義
sample_sizes = pd.Series([5, 10, 50, 100, 500, 1000])

print("各サンプルサイズでの乱数の平均:")
print("=" * 40)

# 各サンプルサイズで平均を計算
results = sample_sizes.apply(calculate_mean)

# 結果を表形式で表示
for size, mean_value in zip(sample_sizes, results):
    print(f"サンプルサイズ {size:4d}: 平均 = {mean_value:.4f}")

print("=" * 40)
print(f"理論値（期待値）: 0.5000")
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample02_map_vector.py
# VS Codeで上記コードを入力
$ python sample02_map_vector.py
```

**期待される出力**:
```
各サンプルサイズでの乱数の平均:
========================================
サンプルサイズ    5: 平均 = 0.4523
サンプルサイズ   10: 平均 = 0.5234
サンプルサイズ   50: 平均 = 0.4987
サンプルサイズ  100: 平均 = 0.5012
サンプルサイズ  500: 平均 = 0.4998
サンプルサイズ 1000: 平均 = 0.5003
========================================
理論値（期待値）: 0.5000
```

サンプルサイズが大きいほど、理論値の0.5に近づくことがわかります。

---

#### サンプル2: 複数のサンプルサイズで統計量を計算（R）

**ファイル名**: `sample02_map_vector.R`

```r
# 複数のサンプルサイズでシミュレーションを実行
library(tidyverse)

# 乱数n個の平均を計算する関数
calculate_mean <- function(n) {
  random_numbers <- runif(n)
  mean(random_numbers)
}

# 複数のサンプルサイズを定義
sample_sizes <- c(5, 10, 50, 100, 500, 1000)

cat("各サンプルサイズでの乱数の平均:\n")
cat(strrep("=", 40), "\n")

# 各サンプルサイズで平均を計算
results <- sample_sizes %>% map_dbl(calculate_mean)

# 結果を表示
for (i in seq_along(sample_sizes)) {
  cat(sprintf("サンプルサイズ %4d: 平均 = %.4f\n", 
              sample_sizes[i], results[i]))
}

cat(strrep("=", 40), "\n")
cat("理論値（期待値）: 0.5000\n")
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample02_map_vector.R
# VS Codeで上記コードを入力
$ Rscript sample02_map_vector.R
```

**期待される出力**:
```
各サンプルサイズでの乱数の平均:
======================================== 
サンプルサイズ    5: 平均 = 0.4523
サンプルサイズ   10: 平均 = 0.5234
サンプルサイズ   50: 平均 = 0.4987
サンプルサイズ  100: 平均 = 0.5012
サンプルサイズ  500: 平均 = 0.4998
サンプルサイズ 1000: 平均 = 0.5003
======================================== 
理論値（期待値）: 0.5000
```

---

### 💡 GitHub Copilot活用ガイド

このセクションで学んだ「1次元データの各要素への関数適用」は、データ分析で非常によく使うパターンです。GitHub Copilotを使って、様々な応用例を試してみましょう。

---

#### 🚀 使えるプロンプト例

##### プロンプト例1: 基本的なマッピング処理 [★☆☆]

**Copilot Chatに入力**:
```
Pythonで、温度のリスト（摂氏）を華氏に変換する関数を作ってください。
その後、pandasのSeriesとapplyメソッドを使って、
複数の摂氏温度[0, 10, 20, 30, 100]を華氏に変換してください。
変換式: F = C × 9/5 + 32
```

**期待される動作**:
- 摂氏→華氏の変換関数
- Seriesとapplyを使った変換
- 結果の表示

**やってみよう**:
1. 生成されたコードを実行
2. 別の温度リストで試す
3. 逆変換（華氏→摂氏）も実装してみる

---

##### プロンプト例2: ファイル処理への応用 [★★☆]

**Copilot Chatに入力**:
```
Pythonで、複数のテキストファイル名のリストがあります。
各ファイルの行数を数える関数を作り、
リスト内包表記を使って全ファイルの行数を取得してください。
ファイル名リスト: ['file1.txt', 'file2.txt', 'file3.txt']
```

**期待される動作**:
- ファイルの行数を数える関数
- 内包表記を使った処理
- エラーハンドリング（ファイルが存在しない場合）

**やってみよう**:
1. テストファイルを作成（各ファイルに数行のテキスト）
2. 生成されたコードを実行
3. ファイルサイズを取得するバージョンも作ってみる

---

##### プロンプト例3: 統計的シミュレーション [★★★]

**Copilot Chatに入力**:
```
Pythonで、中心極限定理を確認するプログラムを作ってください。
1. 一様分布からn個のサンプルを取り、平均を計算する関数
2. サンプルサイズn=[5, 10, 30, 50, 100]で各1000回試行
3. 各サンプルサイズでの平均の標準偏差を計算
4. 理論値（σ/√n）と比較
pandasのSeriesとapplyを使ってください。
```

**期待される動作**:
- サンプル平均を計算する関数
- 各サンプルサイズでの1000回試行
- 標準偏差の計算と理論値との比較
- 結果の可視化（オプション）

**やってみよう**:
1. 生成されたコードを実行
2. 分布を変更（正規分布など）
3. 試行回数を増やして精度を確認

---

#### 📚 Copilot活用のコツ

##### 1. コメントを先に書く
```python
# 各要素に対して複雑な計算を実行
# 入力: サンプルサイズのリスト
# 出力: 各サンプルサイズでの統計量
```

##### 2. 段階的に書く
1. まず1つの要素で動作する関数を定義
2. 小さなリストでテスト
3. 全データに適用
4. 結果を分析

##### 3. 生成されたコードを必ず理解する
- `apply`の引数は何を受け取るか
- 返り値の型は何か
- なぜこの方法が選ばれたか

##### 4. 実験する
- 関数を変更して挙動を確認
- 異なる入力データで試す
- エラーが出たら、原因を探る

---

#### ⚠️ 注意事項

**AIは完璧ではない**
- 特にファイル操作やエラーハンドリングで不完全なコードが生成されることがあります
- 必ず実行して確認しましょう

**理解が第一**
- applyやmap_dblの動作原理を理解しましょう
- 「なぜこの方法を使うのか」を説明できるようになりましょう

**検証する習慣**
- 小さなデータで正しく動くか確認
- エッジケース（空のリスト、大きなデータ）でもテスト

**自分で考える**
- ベクトル化で済む場合は、そちらを優先
- パターン2が本当に必要か、考えてから使う

---

#### 🎓 推奨される学習の流れ

```
1. 教材を読んで「要素ごとの関数適用」を理解
2. sample02_map_vector.py/Rを実行
3. 自分で別の関数を定義して試す
4. Copilotに「〜する関数をapplyで適用」と頼む
5. 生成されたコードを読んで理解
6. パラメータを変えて実験
7. プロンプト例にチャレンジ
```

**大切なのは「AIと協働する」姿勢です。** 一緒に学び、理解を深めていきましょう！

---

## 3.7.3 パターン3: 1次元データ→データフレーム

### 3.7.3.1 概念: 各要素から複数の値を生成

**パターン3**は、1次元データの各要素に対して関数を適用し、**複数の値**（統計量など）を返して、それらをデータフレームにまとめるパターンです。

```
1次元データ (入力)       ⇒   データフレーム (出力)
[5, 10, 100]              n      mean       sd
                          5    平均値    標準偏差
                         10    平均値    標準偏差
                        100    平均値    標準偏差
```

#### どんなときに使うか？

このパターンは、**1つの入力から複数の統計量や結果を計算**し、それらを表形式で整理したいときに使います。例えば：

- 複数のサンプルサイズで、それぞれ平均と標準偏差を計算
- 複数のデータファイルから、行数・列数・欠損値の数を取得
- 複数のパラメータで実験を行い、各種の評価指標を記録

#### パターン2との違い

- **パターン2**: 各要素→1つの値（スカラー）
- **パターン3**: 各要素→複数の値（ベクトル）→データフレームの1行

---

### 3.7.3.2 R: map_dfr関数（行方向結合）

Rでは、`map_dfr()`関数を使います。`dfr`は「data frame by row」（行方向にデータフレームを結合）の略です。

#### 基本構文

```r
入力ベクトル %>% map_dfr(関数)
```

- `入力ベクトル`: 各要素に関数を適用するベクトル
- `関数`: 各要素に適用する関数（**リスト**または**データフレーム**を返す）
- `map_dfr()`: 各関数の結果を行方向に結合してデータフレームを作成

#### 関数が返すべき形式

`map_dfr()`で使う関数は、**名前付きリスト**を返す必要があります。

```r
# 正しい関数の例
my_function <- function(n) {
  list(
    sample_size = n,
    mean_value = 計算結果1,
    sd_value = 計算結果2
  )
}
```

各リストの要素が、データフレームの列になります。

---

### 3.7.3.3 Python: Seriesを返す関数設計

Pythonでは、`apply()`メソッドに、**Seriesを返す関数**を渡します。

#### 基本構文

```python
入力Series.apply(関数)
```

- `入力Series`: 各要素に関数を適用するSeries
- `関数`: 各要素に適用する関数（**pandasのSeries**を返す）

#### 関数が返すべき形式

関数は、**pd.Series**を返す必要があります。インデックス（列名）を指定します。

```python
def my_function(n):
    return pd.Series([
        n,
        計算結果1,
        計算結果2
    ], index=['sample_size', 'mean_value', 'sd_value'])
```

各Seriesの要素が、データフレームの列になります。

---

### 3.7.3.4 実例: サンプルサイズごとに平均と標準偏差を計算

具体例として、「0以上1未満の一様乱数をn個生成し、その平均と標準偏差を計算する」関数を定義します。

#### 標準偏差について

**標準偏差（Standard Deviation, SD）**は、データのばらつきを表す統計量です。値が大きいほど、データが平均から離れています。

- Rでは`sd()`関数
- Pythonでは`np.std(ddof=1)`または`Series.std()`

`ddof=1`は「不偏標準偏差」を計算するためのオプションです（詳細は4.1.1.1項）。

#### 関数の定義

**R版**

```r
f2 <- function(n) {
  # n個の乱数を生成
  tmp <- runif(n)
  
  # 結果をリストで返す
  list(
    x = n,           # サンプルサイズ
    p = mean(tmp),   # 平均
    q = sd(tmp)      # 標準偏差
  )
}

# 動作確認
f2(10)
#> $x
#> [1] 10
#>
#> $p
#> [1] 0.6840032
#>
#> $q
#> [1] 0.3750788
```

**Python版**

```python
import numpy as np
import pandas as pd

def f2(n):
    # n個の乱数を生成
    tmp = np.random.random(n)
    
    # 結果をSeriesで返す
    return pd.Series([
        n,                    # サンプルサイズ
        tmp.mean(),          # 平均
        tmp.std(ddof=1)      # 標準偏差
    ], index=['x', 'p', 'q'])

# 動作確認
f2(10)
#> x    10.000000
#> p     0.405898
#> q     0.317374
#> dtype: float64
```

#### 複数のサンプルサイズで実行

**R版**

```r
v <- c(5, 10, 100)
v %>% map_dfr(f2)
#>       x     p     q
#>   <dbl> <dbl> <dbl>
#> 1     5 0.560 0.320
#> 2    10 0.559 0.271
#> 3   100 0.507 0.283
```

**Python版**

```python
v = pd.Series([5, 10, 100])
v.apply(f2)
#>       x         p         q
#> 0   5.0  0.507798  0.207970
#> 1  10.0  0.687198  0.264427
#> 2 100.0  0.487872  0.280743
```

結果は、3行3列のデータフレームになります。各行が1つのサンプルサイズに対応しています。

---

### 3.7.3.5 結果の解釈

得られたデータフレームから、いくつかの傾向が読み取れます：

1. **平均（p列）**: どのサンプルサイズでも約0.5（理論値は0.5）
2. **標準偏差（q列）**: サンプルサイズが大きいほど、ばらつきが一定になる傾向

サンプルサイズが小さいと、偶然によるばらつきが大きくなります。大きいと、真の分布に近づきます。

---

### 3.7.3.6 実行可能なサンプルプログラム

#### サンプル1: 統計量をデータフレーム化（Python）

**ファイル名**: `sample03_map_dataframe.py`

```python
# 複数のサンプルサイズで統計量を計算し、データフレームにまとめる
import numpy as np
import pandas as pd

# n個の乱数の平均と標準偏差を計算する関数
def calculate_statistics(n):
    # n個の乱数を生成（0以上1未満）
    random_numbers = np.random.random(n)
    
    # 統計量を計算
    mean_val = random_numbers.mean()
    std_val = random_numbers.std(ddof=1)
    
    # Seriesとして返す
    return pd.Series({
        'sample_size': n,
        'mean': mean_val,
        'std_dev': std_val
    })

# 複数のサンプルサイズ
sample_sizes = pd.Series([5, 10, 50, 100, 500, 1000])

# 各サンプルサイズで統計量を計算
print("各サンプルサイズでの統計量:")
print("=" * 50)

results = sample_sizes.apply(calculate_statistics)
print(results)

print("=" * 50)
print("\n統計サマリー:")
print(f"平均の平均: {results['mean'].mean():.4f} (理論値: 0.5000)")
print(f"標準偏差の平均: {results['std_dev'].mean():.4f}")
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample03_map_dataframe.py
# VS Codeで上記コードを入力
$ python sample03_map_dataframe.py
```

**期待される出力**:
```
各サンプルサイズでの統計量:
==================================================
   sample_size      mean   std_dev
0          5.0  0.523456  0.345678
1         10.0  0.487654  0.298765
2         50.0  0.502345  0.287654
3        100.0  0.498765  0.292345
4        500.0  0.500123  0.289012
5       1000.0  0.499876  0.288234
==================================================

統計サマリー:
平均の平均: 0.5020 (理論値: 0.5000)
標準偏差の平均: 0.2917
```

---

#### サンプル2: 統計量をデータフレーム化（R）

**ファイル名**: `sample03_map_dataframe.R`

```r
# 複数のサンプルサイズで統計量を計算し、データフレームにまとめる
library(tidyverse)

# n個の乱数の平均と標準偏差を計算する関数
calculate_statistics <- function(n) {
  # n個の乱数を生成（0以上1未満）
  random_numbers <- runif(n)
  
  # 統計量を計算してリストで返す
  list(
    sample_size = n,
    mean = mean(random_numbers),
    std_dev = sd(random_numbers)
  )
}

# 複数のサンプルサイズ
sample_sizes <- c(5, 10, 50, 100, 500, 1000)

# 各サンプルサイズで統計量を計算
cat("各サンプルサイズでの統計量:\n")
cat(strrep("=", 50), "\n")

results <- sample_sizes %>% map_dfr(calculate_statistics)
print(results)

cat(strrep("=", 50), "\n")
cat("\n統計サマリー:\n")
cat(sprintf("平均の平均: %.4f (理論値: 0.5000)\n", 
            mean(results$mean)))
cat(sprintf("標準偏差の平均: %.4f\n", 
            mean(results$std_dev)))
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample03_map_dataframe.R
# VS Codeで上記コードを入力
$ Rscript sample03_map_dataframe.R
```

**期待される出力**:
```
各サンプルサイズでの統計量:
==================================================
# A tibble: 6 × 3
  sample_size  mean std_dev
        <dbl> <dbl>   <dbl>
1           5 0.523   0.346
2          10 0.488   0.299
3          50 0.502   0.288
4         100 0.499   0.292
5         500 0.500   0.289
6        1000 0.500   0.288
==================================================

統計サマリー:
平均の平均: 0.5020 (理論値: 0.5000)
標準偏差の平均: 0.2917
```

---

### 💡 GitHub Copilot活用ガイド

このセクションで学んだ「1次元データから複数の値を生成してデータフレーム化」は、実験結果の整理やレポート作成で非常に重要なパターンです。GitHub Copilotを使って、様々な応用を試してみましょう。

---

#### 🚀 使えるプロンプト例

##### プロンプト例1: 基本的なデータフレーム生成 [★☆☆]

**Copilot Chatに入力**:
```
Pythonで、数値のリストを受け取り、各数値について以下を計算する関数を作ってください。
- 元の値
- 2乗
- 平方根
関数はpd.Seriesを返すようにしてください。
その後、[4, 9, 16, 25]に対してapplyを使い、データフレームを作成してください。
```

**期待される動作**:
- 複数の計算結果を返す関数
- Seriesを返す設計
- applyを使ったデータフレーム生成

**やってみよう**:
1. 生成されたコードを実行
2. 計算項目を追加（立方根、対数など）
3. 別の数値リストで試す

---

##### プロンプト例2: ファイル情報の集計 [★★☆]

**Copilot Chatに入力**:
```
Pythonで、ファイル名のリストを受け取り、各ファイルについて以下の情報を返す関数を作ってください。
- ファイル名
- ファイルサイズ（バイト）
- 行数
関数はpd.Seriesを返し、pandasのapplyで全ファイルの情報をデータフレームにまとめてください。
エラーハンドリングも含めてください（ファイルが存在しない場合）。
```

**期待される動作**:
- ファイル情報を取得する関数
- エラーハンドリング
- データフレームへの集約

**やってみよう**:
1. テストファイルを3-4個作成
2. 生成されたコードを実行
3. 存在しないファイル名も混ぜて、エラーハンドリングを確認

---

##### プロンプト例3: パラメータスタディ [★★★]

**Copilot Chatに入力**:
```
Pythonで、正規分布からのサンプリング実験を行うプログラムを作ってください。
1. サンプルサイズnを受け取り、以下を計算する関数:
   - サンプルサイズ
   - サンプル平均
   - サンプル標準偏差
   - 理論値からの誤差
   関数はpd.Seriesを返す
2. サンプルサイズ[10, 30, 50, 100, 500]で各項目を計算
3. 結果をデータフレームにまとめて表示
4. 各統計量のトレンドをコメント
```

**期待される動作**:
- 統計実験の関数
- 複数の評価指標
- データフレームへの集約
- 結果の解釈

**やってみよう**:
1. 生成されたコードを実行
2. 分布を変更（t分布、指数分布など）
3. 試行回数を増やして安定性を確認
4. 可視化も追加してみる

---

#### 📚 Copilot活用のコツ

##### 1. コメントを先に書く
```python
# サンプルサイズnに対して複数の統計量を計算
# 戻り値: pd.Series（列名: sample_size, mean, std, min, max）
def calculate_multiple_stats(n):
    # ここにCopilotが提案
```

##### 2. 段階的に書く
1. まず単純な関数（1つの統計量）
2. 複数の統計量を返すように拡張
3. pd.Series形式に変更
4. applyで全体に適用

##### 3. 生成されたコードを必ず理解する
- なぜpd.Seriesを返すのか
- indexパラメータの役割
- applyがどうデータフレームを構築するか

##### 4. 実験する
- 返す統計量を増減
- 異なるデータ型で試す
- エラーケースを確認

---

#### ⚠️ 注意事項

**AIは完璧ではない**
- 特にpd.Seriesの構文で間違いが出やすい
- indexとvaluesの対応を必ず確認

**理解が第一**
- なぜリストではなくSeriesを返すのか
- データフレームがどう構築されるか
- 各行が何を表しているか

**検証する習慣**
- 小さなデータで動作確認
- 統計量が妥当な範囲か確認
- データフレームの構造を確認

**自分で考える**
- どの統計量が必要か、事前に設計
- 列名は意味のある名前に
- 後の分析を考えた設計

---

#### 🎓 推奨される学習の流れ

```
1. 教材を読んで「複数値→データフレーム」を理解
2. sample03_map_dataframe.py/Rを実行
3. 別の統計量（最小値、最大値など）を追加
4. Copilotに「〜する関数をpd.Seriesで」と頼む
5. 生成されたコードを読んで理解
6. 実データで試す
7. プロンプト例にチャレンジ
```

**大切なのは「AIと協働する」姿勢です。** データフレームの構造を理解しながら進めましょう！

---

## 3.7.4 パターン4: データフレーム→データフレーム

### 3.7.4.1 概念: 各行に多引数関数を適用

**パターン4**は、データフレームの各行に対して関数を適用し、結果を新しいデータフレームにまとめるパターンです。各行の複数の列を、関数の複数の引数として渡します。

```
データフレーム (入力)         ⇒       データフレーム (出力)
  x    y                            x    y      mean      sd
  5    6                            5    6     平均値  標準偏差
 10    6                           10    6     平均値  標準偏差
100    6                          100    6     平均値  標準偏差
  5   12                            5   12     平均値  標準偏差
 10   12                           10   12     平均値  標準偏差
100   12                          100   12     平均値  標準偏差
```

#### どんなときに使うか？

このパターンは、**複数のパラメータの組み合わせ**で実験を行うときに使います。例えば：

- パラメータxとyの様々な組み合わせで計算を実行
- 各行に記録された条件で、シミュレーションを実行
- グリッドサーチ（パラメータの網羅的な探索）

#### パターン3との違い

- **パターン3**: 1次元データの各要素→1つの引数→複数の結果
- **パターン4**: データフレームの各行→**複数の引数**→複数の結果

---

### 3.7.4.2 R: pmap_dfr関数

Rでは、`pmap_dfr()`関数を使います。`p`は「parallel」（並行）の意味で、複数の列を並行して処理します。

#### 基本構文

```r
データフレーム %>% pmap_dfr(関数)
```

- `データフレーム`: 各行の列が関数の引数になる
- `関数`: 各行に適用する関数（引数は列名と対応）
- `pmap_dfr()`: 各関数の結果を行方向に結合

#### 動作の仕組み

`pmap_dfr()`は、データフレームの各行を取り出し、列の値を関数の引数として渡します。

```r
# データフレーム
df <- data.frame(x = c(1, 2), y = c(3, 4))

# 関数（xとyを受け取る）
my_func <- function(x, y) {
  list(x = x, y = y, sum = x + y)
}

# 適用
df %>% pmap_dfr(my_func)
#>   x y sum
#> 1 1 3   4
#> 2 2 4   6
```

---

### 3.7.4.3 Python: applyとlambda式

Pythonでは、`apply()`メソッドに`axis=1`を指定し、lambda式を使います。

#### 基本構文

```python
データフレーム.apply(lambda row: 関数(row['列1'], row['列2']), axis=1)
```

- `axis=1`: 行方向に適用（各行が`row`として渡される）
- `lambda row`: 各行を受け取る無名関数
- `row['列名']`: 行から特定の列を取り出す

#### lambda式とは？

**lambda式**（ラムダ式）は、名前のない小さな関数を定義する方法です。

```python
# 通常の関数定義
def add(x, y):
    return x + y

# lambda式（同じ機能）
add = lambda x, y: x + y
```

`apply`と組み合わせることで、各行のデータを関数に渡せます。

#### 簡潔な書き方

列名と引数名が一致している場合、アンパック演算子`*`が使えます。

```python
# 詳細な書き方
データフレーム.apply(lambda row: 関数(row['x'], row['y']), axis=1)

# 簡潔な書き方
データフレーム.apply(lambda row: 関数(*row), axis=1)
```

`*row`は、行の全要素を順番に引数として展開します。

---

### 3.7.4.4 実例: パラメータx, yで乱数実験

具体例として、「1以上y以下の整数の一様乱数をx個生成し、その平均と標準偏差を計算する」関数を定義します。

#### 関数の定義

**R版**

```r
f3 <- function(x, y) {
  # x個の乱数を生成（1以上y以下の整数）
  tmp <- runif(x, min = 1, max = y + 1) %>%
    as.integer()
  
  # 結果をリストで返す
  list(
    x = x,
    y = y,
    p = mean(tmp),
    q = sd(tmp)
  )
}

# 動作確認
f3(x = 10, y = 6)
#> $x
#> [1] 10
#>
#> $y
#> [1] 6
#>
#> $p
#> [1] 3.2
#>
#> $q
#> [1] 1.316561
```

**Python版**

```python
import numpy as np
import pandas as pd

def f3(x, y):
    # x個の乱数を生成（1以上y以下の整数）
    tmp = np.random.randint(1, y + 1, x)
    
    # 結果をSeriesで返す
    return pd.Series({
        'x': x,
        'y': y,
        'p': tmp.mean(),
        'q': tmp.std(ddof=1)
    })

# 動作確認
f3(10, 6)
#> x    10.000000
#> y     6.000000
#> p     3.200000
#> q     1.316561
#> dtype: float64
```

#### パラメータの組み合わせで実行

様々なxとyの組み合わせを試します。

**R版**

```r
# パラメータの組み合わせを定義
my_df <- data.frame(
  x = c(5, 10, 100,  5, 10, 100),
  y = c(6,  6,   6, 12, 12,  12)
)

# 各行に関数を適用
my_df %>% pmap_dfr(f3)
#>       x     y     p     q
#>   <dbl> <dbl> <dbl> <dbl>
#> 1     5     6  3.00  1.41
#> 2    10     6  3.00  1.49
#> 3   100     6  3.57  1.78
#> 4     5    12  5.20  3.77
#> 5    10    12  5.70  3.77
#> 6   100    12  6.36  3.59
```

**Python版**

```python
# パラメータの組み合わせを定義
my_df = pd.DataFrame({
    'x': [5, 10, 100, 5, 10, 100],
    'y': [6, 6, 6, 12, 12, 12]
})

# 各行に関数を適用
my_df.apply(lambda row: f3(row['x'], row['y']), axis=1)

# または簡潔に
# my_df.apply(lambda row: f3(*row), axis=1)

#>        x     y     p     q
#> 0    5.0   6.0  3.37  1.96
#> 1   10.0   6.0  1.92  0.95
#> 2  100.0   6.0  2.90  1.73
#> 3    5.0  12.0  6.82  3.00
#> 4   10.0  12.0  7.05  2.42
#> 5  100.0  12.0  5.90  3.54
```

---

### 3.7.4.5 結果の解釈

得られた結果から、以下のことがわかります：

1. **yの影響**: yが大きいほど、平均（p）も大きくなる
   - y=6のとき: 平均は約3.5（理論値: (1+6)/2 = 3.5）
   - y=12のとき: 平均は約6.5（理論値: (1+12)/2 = 6.5）

2. **xの影響**: xが大きいほど、理論値に近づく
   - x=5: ばらつきが大きい
   - x=100: より理論値に近い

3. **標準偏差（q）**: yが大きいほど、標準偏差も大きい
   - 範囲が広いほど、データのばらつきも大きい

---

### 3.7.4.6 実行可能なサンプルプログラム

#### サンプル1: パラメータグリッドで実験（Python）

**ファイル名**: `sample04_pmap.py`

```python
# データフレームの各行のパラメータで実験を実行
import numpy as np
import pandas as pd

# xとyを受け取り、統計量を計算する関数
def run_experiment(x, y):
    """
    x個の整数乱数（1以上y以下）を生成し、統計量を計算
    """
    # 乱数生成
    random_integers = np.random.randint(1, y + 1, x)
    
    # 統計量を計算
    return pd.Series({
        'sample_size': x,
        'max_value': y,
        'mean': random_integers.mean(),
        'std_dev': random_integers.std(ddof=1),
        'theoretical_mean': (1 + y) / 2
    })

# パラメータグリッドを作成
# sample_sizeとmax_valueの組み合わせ
param_grid = pd.DataFrame({
    'x': [10, 10, 10, 100, 100, 100],
    'y': [6, 12, 20, 6, 12, 20]
})

print("パラメータグリッド:")
print(param_grid)
print("\n" + "=" * 60)

# 各行で実験を実行
print("\n実験結果:")
results = param_grid.apply(
    lambda row: run_experiment(row['x'], row['y']),
    axis=1
)

print(results)
print("\n" + "=" * 60)

# 理論値との比較
print("\n理論値からの誤差:")
results['error'] = abs(results['mean'] - results['theoretical_mean'])
print(results[['sample_size', 'max_value', 'mean', 'theoretical_mean', 'error']])
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample04_pmap.py
# VS Codeで上記コードを入力
$ python sample04_pmap.py
```

**期待される出力**:
```
パラメータグリッド:
     x   y
0   10   6
1   10  12
2   10  20
3  100   6
4  100  12
5  100  20

============================================================

実験結果:
   sample_size  max_value      mean   std_dev  theoretical_mean
0         10.0        6.0  3.200000  1.398412               3.5
1         10.0       12.0  6.800000  3.584569               6.5
2         10.0       20.0 10.100000  5.912021              10.5
3        100.0        6.0  3.520000  1.722640               3.5
4        100.0       12.0  6.480000  3.446789               6.5
5        100.0       20.0 10.510000  5.778901              10.5

============================================================

理論値からの誤差:
   sample_size  max_value      mean  theoretical_mean     error
0         10.0        6.0  3.200000               3.5  0.300000
1         10.0       12.0  6.800000               6.5  0.300000
2         10.0       20.0 10.100000              10.5  0.400000
3        100.0        6.0  3.520000               3.5  0.020000
4        100.0       12.0  6.480000               6.5  0.020000
5        100.0       20.0 10.510000              10.5  0.010000
```

サンプルサイズが大きいほど、誤差が小さくなることが確認できます。

---

#### サンプル2: パラメータグリッドで実験（R）

**ファイル名**: `sample04_pmap.R`

```r
# データフレームの各行のパラメータで実験を実行
library(tidyverse)

# xとyを受け取り、統計量を計算する関数
run_experiment <- function(x, y) {
  # 乱数生成
  random_integers <- runif(x, min = 1, max = y + 1) %>%
    as.integer()
  
  # 統計量を計算してリストで返す
  list(
    sample_size = x,
    max_value = y,
    mean = mean(random_integers),
    std_dev = sd(random_integers),
    theoretical_mean = (1 + y) / 2
  )
}

# パラメータグリッドを作成
param_grid <- data.frame(
  x = c(10, 10, 10, 100, 100, 100),
  y = c(6, 12, 20, 6, 12, 20)
)

cat("パラメータグリッド:\n")
print(param_grid)
cat("\n", strrep("=", 60), "\n")

# 各行で実験を実行
cat("\n実験結果:\n")
results <- param_grid %>% pmap_dfr(run_experiment)
print(results)

cat("\n", strrep("=", 60), "\n")

# 理論値との比較
cat("\n理論値からの誤差:\n")
results <- results %>%
  mutate(error = abs(mean - theoretical_mean))

print(results %>% select(sample_size, max_value, mean, theoretical_mean, error))
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample04_pmap.R
# VS Codeで上記コードを入力
$ Rscript sample04_pmap.R
```

**期待される出力**:
```
パラメータグリッド:
    x  y
1  10  6
2  10 12
3  10 20
4 100  6
5 100 12
6 100 20

 ============================================================ 

実験結果:
# A tibble: 6 × 5
  sample_size max_value  mean std_dev theoretical_mean
        <dbl>     <dbl> <dbl>   <dbl>            <dbl>
1          10         6  3.2    1.40               3.5
2          10        12  6.8    3.58               6.5
3          10        20 10.1    5.91              10.5
4         100         6  3.52   1.72               3.5
5         100        12  6.48   3.45               6.5
6         100        20 10.5    5.78              10.5

 ============================================================ 

理論値からの誤差:
# A tibble: 6 × 5
  sample_size max_value  mean theoretical_mean error
        <dbl>     <dbl> <dbl>            <dbl> <dbl>
1          10         6  3.2               3.5  0.3 
2          10        12  6.8               6.5  0.3 
3          10        20 10.1              10.5  0.4 
4         100         6  3.52              3.5  0.02
5         100        12  6.48              6.5  0.02
6         100        20 10.5              10.5  0.01
```

---

### 💡 GitHub Copilot活用ガイド

このセクションで学んだ「データフレームの各行への多引数関数適用」は、パラメータスタディや実験計画で中心的な役割を果たします。GitHub Copilotを使って、複雑なパラメータグリッドの処理を効率化しましょう。

---

#### 🚀 使えるプロンプト例

##### プロンプト例1: 基本的なパラメータグリッド [★★☆]

**Copilot Chatに入力**。
```
Pythonで、2つのパラメータa, bを受け取り、a^b（aのb乗）を計算する関数を作ってください。
その後、以下のパラメータグリッドで計算を実行してください。
a = [2, 3, 5]
b = [2, 3, 4]
全ての組み合わせ（9通り）を試し、結果をデータフレームにまとめてください。
applyとlambdaを使ってください。
```

**期待される動作**:
- べき乗計算の関数
- パラメータグリッドの作成
- apply(axis=1)を使った適用
- 結果のデータフレーム

**やってみよう**:
1. 生成されたコードを実行
2. 別の計算式で試す（積、商など）
3. パラメータの範囲を変更

---

##### プロンプト例2: 機械学習のハイパーパラメータ探索 [★★★]

**Copilot Chatに入力**:
```
Pythonで、決定木の深さ(max_depth)と最小サンプル数(min_samples)を変えながら、
モデルの精度を評価するプログラムを作ってください。
1. max_depth = [3, 5, 10], min_samples = [2, 5, 10]の全組み合わせ
2. 各組み合わせでモデルを訓練し、精度を計算
3. 結果をデータフレームにまとめる
4. 最良のパラメータを表示
scikit-learnのirisデータセットを使い、applyとlambdaで実装してください。
```

**期待される動作**:
- パラメータグリッドの作成
- 各組み合わせでモデル訓練
- 精度の計算と記録
- 最良パラメータの特定

**やってみよう**:
1. 生成されたコードを実行
2. 別のアルゴリズム（ロジスティック回帰など）で試す
3. 交差検証を追加
4. 結果を可視化

---

##### プロンプト例3: シミュレーション実験の設計 [★★★]

**Copilot Chatに入力**:
```
Pythonで、感染症の簡易シミュレーションを行うプログラムを作ってください。
パラメータ:
- population: 人口 [100, 500, 1000]
- infection_rate: 感染率 [0.1, 0.3, 0.5]
- recovery_days: 回復日数 [7, 14, 21]

各パラメータの組み合わせで、30日間のシミュレーションを実行し、
以下を記録してください:
- 最大感染者数
- 感染者数がピークに達する日
- 最終的な感染者の累計

結果をデータフレームにまとめ、applyで実装してください。
```

**期待される動作**:
- シミュレーション関数
- 大規模なパラメータグリッド
- 複数の評価指標
- 結果の整理と分析

**やってみよう**:
1. 生成されたコードを実行
2. パラメータを変更して傾向を観察
3. 可視化を追加（感染者数の推移）
4. 最も危険なパラメータの組み合わせを特定

---

#### 📚 Copilot活用のコツ

##### 1. コメントを先に書く
```python
# パラメータxとyを受け取り、実験を実行
# 戻り値: pd.Series（パラメータと結果を含む）
def run_experiment(x, y):
    # ここにCopilotが提案
```

##### 2. 段階的に書く
1. まず関数を1組のパラメータでテスト
2. データフレームを作成
3. 1行だけapplyでテスト
4. 全行に適用
5. 結果を分析

##### 3. 生成されたコードを必ず理解する
- lambda式の構文
- axis=1の意味
- row['列名']での列アクセス
- `*row`によるアンパック

##### 4. 実験する
- パラメータの組み合わせを変更
- 関数の中身を変更
- 結果の可視化

---

#### ⚠️ 注意事項

**AIは完璧ではない**
- lambda式の構文エラーが出やすい
- 列名と引数名の対応を確認
- axis=1を忘れないように

**理解が第一**
- applyがどう各行を処理するか
- lambda式がどう動くか
- データフレームの構造

**検証する習慣**
- 小さなグリッドでテスト
- 1行だけ実行して確認
- 結果が妥当か検証

**自分で考える**
- どのパラメータが必要か
- どの評価指標を記録するか
- 効率的なグリッドの設計

---

#### 🎓 推奨される学習の流れ

```
1. 教材を読んで「データフレーム→データフレーム」を理解
2. sample04_pmap.py/Rを実行
3. 別のパラメータの組み合わせを試す
4. Copilotに「xとyを使う関数を適用」と頼む
5. 生成されたlambda式を理解
6. 大きなグリッドで実験
7. プロンプト例にチャレンジ
```

**大切なのは「AIと協働する」姿勢です。** パラメータスタディの設計力を高めましょう！

---

## 3.7.5 並列処理入門

### 3.7.5.1 並列処理とは何か

**並列処理（Parallel Processing）**とは、複数の処理を同時に実行することです。コンピュータの複数のCPUコア（プロセッサ）を活用して、計算を高速化します。

#### 直列処理 vs 並列処理

**直列処理（通常の方法）**:
```
タスク1 → タスク2 → タスク3 → タスク4  （順番に実行）
時間: 4秒
```

**並列処理**:
```
タスク1 ┐
タスク2 ├→ 同時実行
タスク3 ┤
タスク4 ┘
時間: 1秒（4コアの場合）
```

---

### 3.7.5.2 並列処理の利点と注意点

#### 利点

1. **計算時間の短縮**: 大量のデータや複雑な計算で効果大
2. **リソースの有効活用**: 現代のPCは複数のコアを持っている
3. **スケーラビリティ**: データ量が増えても対応しやすい

#### 注意点

1. **独立性が必要**: 各処理が互いに影響しないこと
2. **オーバーヘッド**: 並列化自体にコストがかかる
3. **デバッグの難しさ**: エラーの原因特定が難しい
4. **乱数の扱い**: シードの設定が重要

#### いつ並列処理を使うべきか？

**使うべき場合**:
- 大量の独立した計算（数千〜数万回のシミュレーション）
- 各処理に時間がかかる（1回あたり数秒以上）
- データ量が多い

**使わない方が良い場合**:
- 処理が少ない（数十回程度）
- 各処理が瞬時に終わる（ミリ秒単位）
- 処理間に依存関係がある

---

### 3.7.5.3 並列化できる処理の条件

本節で紹介している反復処理は、以下の理由で並列化に適しています：

1. **独立性**: 各処理が他の処理に影響しない
2. **データ並列**: 同じ関数を異なるデータに適用
3. **状態なし**: グローバル変数などを変更しない

例えば、3.7.2節のパターン2:
```python
# 各要素の処理は完全に独立
v = pd.Series([5, 10, 100])
v.apply(f1)  # f1(5), f1(10), f1(100)は互いに影響しない
```

---

### 3.7.5.4 R: furrr パッケージによる並列処理

Rでは、`furrr`パッケージを使います。`future`パッケージに基づく並列処理のフレームワークです。

#### 基本的な使い方

**ステップ1: パッケージのインストールと読み込み**

```r
# 初回のみ（コメントを外して実行）
# install.packages("furrr")

library(furrr)
```

**ステップ2: 並列処理の準備**

```r
# 並列処理を有効化
plan(multisession)
```

`multisession`は、複数のRセッションを起動して並列処理を行います。

**ステップ3: 関数名を変更**

通常の関数名の前に`future_`を付けます。

| 通常の関数 | 並列版 |
|:----------|:------|
| `map_dbl()` | `future_map_dbl()` |
| `map_dfr()` | `future_map_dfr()` |
| `pmap_dfr()` | `future_pmap_dfr()` |

**ステップ4: 乱数の設定**

乱数を使う場合、`.options`引数で設定します。

```r
v %>% future_map_dbl(f1, .options = furrr_options(seed = TRUE))
```

`seed = TRUE`により、再現可能な乱数が生成されます。

---

### 3.7.5.5 Python: pandarallel による並列処理

Pythonでは、`pandarallel`パッケージを使います。pandasの`apply`を並列化します。

#### 基本的な使い方

**ステップ1: パッケージのインストールと読み込み**

```bash
# 初回のみ（ターミナルで実行）
$ pip install pandarallel --break-system-packages
```

```python
from pandarallel import pandarallel
```

**ステップ2: 並列処理の準備**

```python
# 並列処理を初期化
pandarallel.initialize()
```

初期化時にプログレスバーが表示されます。

**ステップ3: メソッド名を変更**

`apply`の代わりに`parallel_apply`を使います。

| 通常のメソッド | 並列版 |
|:-------------|:------|
| `.apply()` | `.parallel_apply()` |

**使用例**:

```python
# 通常の方法
v = pd.Series([5, 10, 100])
v.apply(f1)

# 並列処理
v.parallel_apply(f1)
```

---

### 3.7.5.6 実例: 並列処理の比較

3.7.2節のパターン2を並列化してみます。

#### R版

```r
library(tidyverse)
library(furrr)

# 関数の定義
f1 <- function(x) {
  tmp <- runif(x)
  mean(tmp)
}

# 並列処理の準備
plan(multisession)

# 通常の処理
v <- c(5, 10, 100)
system.time(v %>% map_dbl(f1))

# 並列処理
system.time(v %>% future_map_dbl(f1, .options = furrr_options(seed = TRUE)))
```

**注意**: サンプルサイズが小さい場合、並列化のオーバーヘッドにより、かえって遅くなることがあります。

#### Python版

```python
import numpy as np
import pandas as pd
from pandarallel import pandarallel
import time

# 関数の定義
def f1(x):
    tmp = np.random.random(x)
    return np.mean(tmp)

# 並列処理の準備
pandarallel.initialize()

# テストデータ
v = pd.Series([5, 10, 100])

# 通常の処理
start = time.time()
result1 = v.apply(f1)
print(f"通常: {time.time() - start:.4f}秒")

# 並列処理
start = time.time()
result2 = v.parallel_apply(f1)
print(f"並列: {time.time() - start:.4f}秒")
```

---

### 3.7.5.7 並列処理の効果を実感する例

小さなサンプルでは並列化の効果が見えにくいので、もう少し大きなデータで試します。

#### R版

**ファイル名**: `sample05_parallel.R`

```r
# 並列処理の効果を確認
library(tidyverse)
library(furrr)

# 重い計算を行う関数
heavy_calculation <- function(n) {
  # n個の乱数で複雑な計算
  x <- runif(n)
  y <- runif(n)
  
  # 相関係数を計算（少し時間がかかる）
  cor(x, y)
}

# 並列処理の準備
plan(multisession)

# 大きめのサンプルサイズ
sample_sizes <- rep(10000, 100)  # 10000個の乱数を100回

cat("通常の処理:\n")
start_time <- Sys.time()
results1 <- sample_sizes %>% map_dbl(heavy_calculation)
time1 <- Sys.time() - start_time
cat(sprintf("実行時間: %.2f秒\n", as.numeric(time1)))

cat("\n並列処理:\n")
start_time <- Sys.time()
results2 <- sample_sizes %>% 
  future_map_dbl(heavy_calculation, .options = furrr_options(seed = TRUE))
time2 <- Sys.time() - start_time
cat(sprintf("実行時間: %.2f秒\n", as.numeric(time2)))

cat(sprintf("\n高速化率: %.2f倍\n", as.numeric(time1) / as.numeric(time2)))
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample05_parallel.R
# VS Codeで上記コードを入力
$ Rscript sample05_parallel.R
```

**期待される出力**:
```
通常の処理:
実行時間: 8.45秒

並列処理:
実行時間: 2.31秒

高速化率: 3.66倍
```
（環境により異なります）

---

#### Python版

**ファイル名**: `sample05_parallel.py`

```python
# 並列処理の効果を確認
import numpy as np
import pandas as pd
from pandarallel import pandarallel
import time

# 重い計算を行う関数
def heavy_calculation(n):
    """n個の乱数で複雑な計算"""
    # n個の乱数を2セット生成
    x = np.random.random(n)
    y = np.random.random(n)
    
    # 相関係数を計算（少し時間がかかる）
    return np.corrcoef(x, y)[0, 1]

# 並列処理の準備
pandarallel.initialize(progress_bar=False)

# 大きめのサンプルサイズ
sample_sizes = pd.Series([10000] * 100)  # 10000個の乱数を100回

print("通常の処理:")
start_time = time.time()
results1 = sample_sizes.apply(heavy_calculation)
time1 = time.time() - start_time
print(f"実行時間: {time1:.2f}秒")

print("\n並列処理:")
start_time = time.time()
results2 = sample_sizes.parallel_apply(heavy_calculation)
time2 = time.time() - start_time
print(f"実行時間: {time2:.2f}秒")

print(f"\n高速化率: {time1 / time2:.2f}倍")
```

**実行方法**:
```bash
$ cd ~/work
$ touch sample05_parallel.py
# VS Codeで上記コードを入力
$ python sample05_parallel.py
```

**期待される出力**:
```
通常の処理:
実行時間: 8.45秒

並列処理:
実行時間: 2.31秒

高速化率: 3.66倍
```
（環境により異なります）

---

### 3.7.5.8 並列処理のトラブルシューティング

#### よくある問題と対処法

**1. 並列処理が遅い**
- 原因: タスクが小さすぎる、並列化のオーバーヘッド
- 対処: より大きなタスクで試す、通常の処理と比較

**2. エラーが出る**
- 原因: 関数内でグローバル変数を参照、外部ファイルにアクセス
- 対処: 関数を自己完結型にする、必要なパッケージを関数内で読み込む

**3. 結果が再現しない（乱数）**
- R: `.options = furrr_options(seed = TRUE)`を指定
- Python: 各ワーカーで独立したシードが設定される

**4. メモリ不足**
- 原因: 各プロセスがメモリを消費
- 対処: ワーカー数を減らす、データを小さくする

---

### 💡 GitHub Copilot活用ガイド

並列処理は少し高度なトピックですが、GitHub Copilotを使うことで、効率的に学習できます。まずは小さな例から始めて、徐々に大きな問題に挑戦しましょう。

---

#### 🚀 使えるプロンプト例

##### プロンプト例1: 基本的な並列処理 [★★☆]

**Copilot Chatに入力**:
```
Pythonで、フィボナッチ数列のn番目を計算する関数を作ってください（再帰的に）。
その後、n = [30, 31, 32, 33, 34, 35]に対して計算を行い、
通常のapplyと並列処理（pandarallel）で実行時間を比較してください。
```

**期待される動作**:
- フィボナッチ関数（計算に時間がかかる）
- 通常の処理
- 並列処理
- 実行時間の比較

**やってみよう**:
1. 生成されたコードを実行
2. nの値を増やして効果を確認
3. 他の重い計算で試す

---

##### プロンプト例2: 大規模シミュレーションの並列化 [★★★]

**Copilot Chatに入力**:
```
Pythonで、モンテカルロ法を使ってπを推定する関数を作ってください。
試行回数nを受け取り、n個の点を使ってπを推定します。
その後、n = 100000で1000回のシミュレーションを実行し、
通常のapplyと並列処理で実行時間を比較してください。
推定値の平均と標準偏差も表示してください。
```

**期待される動作**:
- モンテカルロ法の実装
- 大規模な繰り返し
- 並列化による高速化
- 統計的な分析

**やってみよう**:
1. 生成されたコードを実行
2. 試行回数を変更して精度を確認
3. 繰り返し回数を増やして高速化率を確認
4. 結果の分布を可視化

---

##### プロンプト例3: パラメータグリッドの並列探索 [★★★]

**Copilot Chatに入力**:
```
Pythonで、以下のパラメータグリッドで機械学習モデルを訓練し、
最良のパラメータを見つけるプログラムを作ってください。
- max_depth: [3, 5, 7, 10, 15, 20]
- min_samples_split: [2, 5, 10, 20]
- min_samples_leaf: [1, 2, 4, 8]

全ての組み合わせ（72通り）を試し、並列処理で高速化してください。
scikit-learnのirisデータセットと決定木を使い、
交差検証スコアを評価指標としてください。
```

**期待される動作**:
- パラメータグリッドの作成
- 各組み合わせでモデル訓練と評価
- 並列処理による高速化
- 最良パラメータの特定

**やってみよう**:
1. 生成されたコードを実行
2. 別のアルゴリズムで試す
3. パラメータの範囲を拡張
4. 結果を可視化（ヒートマップなど）

---

#### 📚 Copilot活用のコツ

##### 1. コメントを先に書く
```python
# 重い計算を行う関数（並列化のテスト用）
# 引数: n (計算の大きさ)
# 戻り値: 計算結果
def heavy_calculation(n):
    # ここにCopilotが提案
```

##### 2. 段階的に書く
1. まず通常の処理で動作確認
2. 小さなデータで並列化をテスト
3. 大きなデータで高速化を確認
4. エラー処理を追加

##### 3. 生成されたコードを必ず理解する
- 並列化の準備（initialize, plan）
- 関数名の変更（apply → parallel_apply）
- オーバーヘッドと高速化のトレードオフ

##### 4. 実験する
- タスクの大きさを変える
- 繰り返し回数を変える
- 実行時間を計測して比較

---

#### ⚠️ 注意事項

**AIは完璧ではない**
- 並列処理の適切な使い所を判断できないことがある
- エラーハンドリングが不完全なことがある

**理解が第一**
- なぜ並列化で速くなるのか
- いつ並列化すべきか
- オーバーヘッドとは何か

**検証する習慣**
- 必ず実行時間を計測
- 結果が正しいか確認
- 小さなデータでデバッグ

**自分で考える**
- 本当に並列化が必要か
- 通常の最適化で十分ではないか
- コードの可読性とのバランス

---

#### 🎓 推奨される学習の流れ

```
1. 教材を読んで並列処理の概念を理解
2. sample05_parallel.py/Rを実行
3. タスクの大きさを変えて効果を確認
4. Copilotに「〜を並列化して」と頼む
5. 生成されたコードを読んで理解
6. 実行時間を計測して比較
7. プロンプト例にチャレンジ
```

**大切なのは「AIと協働する」姿勢です。** 並列処理を適切に活用し、効率的な計算を実現しましょう！

---

## 3.7.6 統合演習

### 3.7.6.1 演習の目的

これまで学んだ4つのパターンを統合的に活用し、実践的なデータ分析タスクに取り組みます。

**演習で扱うこと**:
1. 複数のパターンを組み合わせる
2. Python/R両方で実装する
3. 結果を分析・解釈する
4. GitHub Copilotを活用する

---

### 3.7.6.2 演習課題1: モンテカルロシミュレーション

#### 課題の説明

**テーマ**: モンテカルロ法を使って円周率πを推定し、推定精度をサンプルサイズで比較する

**背景知識**: モンテカルロ法は、ランダムな試行を繰り返すことで数値を推定する方法です。円周率の推定では、正方形内にランダムに点を打ち、円の内側に入る確率からπを推定します。

```
正方形の面積: 4 (辺の長さ2)
円の面積: π (半径1)
円の内側に入る確率 ≈ π/4
→ π ≈ 4 × (内側の点数 / 全点数)
```

#### 要件

**入力**:
- サンプルサイズのリスト: `[100, 500, 1000, 5000, 10000]`
- 各サンプルサイズで10回試行

**出力データフレーム**:
```
sample_size  trial  pi_estimate  error
100          1      3.12         0.0216
100          2      3.16         0.0184
...
```

**使用するパターン**:
1. パターン2または3: サンプルサイズごとに推定
2. パターン1: 各サンプルサイズで10回繰り返し

---

#### ファイル構成

**Python版**: `exercise01_monte_carlo.py`  
**R版**: `exercise01_monte_carlo.R`

---

#### ヒント

1. **点が円の内側にあるか判定**:
   - 点(x, y)が原点からの距離が1以下なら内側
   - 距離 = √(x² + y²)

2. **パターンの組み合わせ**:
   - 外側: サンプルサイズのループ
   - 内側: 各サンプルサイズで10回試行

3. **エラーの計算**:
   - error = |推定値 - π|
   - πは`math.pi`(Python)または`pi`(R)

---

#### 解答例（Python）

**ファイル名**: `exercise01_monte_carlo.py`

```python
# モンテカルロ法による円周率の推定
import numpy as np
import pandas as pd
import math

# 1回の推定を行う関数
def estimate_pi(n):
    """
    n個のランダムな点を使ってπを推定
    """
    # -1から1の範囲でランダムな点を生成
    x = np.random.uniform(-1, 1, n)
    y = np.random.uniform(-1, 1, n)
    
    # 原点からの距離が1以下（円の内側）か判定
    inside_circle = (x**2 + y**2) <= 1
    
    # π = 4 × (円の内側の点数 / 全点数)
    pi_estimate = 4 * np.sum(inside_circle) / n
    
    return pi_estimate

# 各サンプルサイズで10回試行する関数
def run_trials(sample_size, num_trials=10):
    """
    指定したサンプルサイズで複数回試行
    """
    results = []
    for trial in range(1, num_trials + 1):
        pi_est = estimate_pi(sample_size)
        error = abs(pi_est - math.pi)
        results.append({
            'sample_size': sample_size,
            'trial': trial,
            'pi_estimate': pi_est,
            'error': error
        })
    return pd.DataFrame(results)

# サンプルサイズのリスト
sample_sizes = [100, 500, 1000, 5000, 10000]

print("モンテカルロ法による円周率の推定")
print("=" * 60)
print(f"真の値: π = {math.pi:.6f}\n")

# 全てのサンプルサイズで実験
all_results = []
for size in sample_sizes:
    print(f"サンプルサイズ {size:5d} で実験中...")
    df = run_trials(size)
    all_results.append(df)

# 結果を結合
final_results = pd.concat(all_results, ignore_index=True)

print("\n" + "=" * 60)
print("各サンプルサイズでの統計:")
print("=" * 60)

# サンプルサイズごとの統計量
summary = final_results.groupby('sample_size').agg({
    'pi_estimate': ['mean', 'std'],
    'error': ['mean', 'min', 'max']
}).round(6)

print(summary)

print("\n" + "=" * 60)
print("結論:")
print("サンプルサイズが大きいほど、推定精度が向上します。")
print("誤差の平均と標準偏差が減少していることが確認できます。")
```

**実行方法**:
```bash
$ cd ~/work
$ touch exercise01_monte_carlo.py
# VS Codeで上記コードを入力
$ python exercise01_monte_carlo.py
```

---

#### 解答例（R）

**ファイル名**: `exercise01_monte_carlo.R`

```r
# モンテカルロ法による円周率の推定
library(tidyverse)

# 1回の推定を行う関数
estimate_pi <- function(n) {
  # -1から1の範囲でランダムな点を生成
  x <- runif(n, min = -1, max = 1)
  y <- runif(n, min = -1, max = 1)
  
  # 原点からの距離が1以下（円の内側）か判定
  inside_circle <- (x^2 + y^2) <= 1
  
  # π = 4 × (円の内側の点数 / 全点数)
  pi_estimate <- 4 * sum(inside_circle) / n
  
  pi_estimate
}

# 各サンプルサイズで10回試行する関数
run_trials <- function(sample_size, num_trials = 10) {
  results <- replicate(
    n = num_trials,
    expr = estimate_pi(sample_size)
  )
  
  data.frame(
    sample_size = sample_size,
    trial = 1:num_trials,
    pi_estimate = results,
    error = abs(results - pi)
  )
}

# サンプルサイズのリスト
sample_sizes <- c(100, 500, 1000, 5000, 10000)

cat("モンテカルロ法による円周率の推定\n")
cat(strrep("=", 60), "\n")
cat(sprintf("真の値: π = %.6f\n\n", pi))

# 全てのサンプルサイズで実験
all_results <- sample_sizes %>%
  map_dfr(~ {
    cat(sprintf("サンプルサイズ %5d で実験中...\n", .x))
    run_trials(.x)
  })

cat("\n", strrep("=", 60), "\n")
cat("各サンプルサイズでの統計:\n")
cat(strrep("=", 60), "\n")

# サンプルサイズごとの統計量
summary <- all_results %>%
  group_by(sample_size) %>%
  summarise(
    mean_estimate = mean(pi_estimate),
    sd_estimate = sd(pi_estimate),
    mean_error = mean(error),
    min_error = min(error),
    max_error = max(error)
  )

print(summary, n = Inf)

cat("\n", strrep("=", 60), "\n")
cat("結論:\n")
cat("サンプルサイズが大きいほど、推定精度が向上します。\n")
cat("誤差の平均と標準偏差が減少していることが確認できます。\n")
```

**実行方法**:
```bash
$ cd ~/work
$ touch exercise01_monte_carlo.R
# VS Codeで上記コードを入力
$ Rscript exercise01_monte_carlo.R
```

---

### 3.7.6.3 演習課題2: データ分析パイプライン

#### 課題の説明

**テーマ**: 複数の統計分布からサンプルを生成し、各分布の特性を比較する

**扱う分布**:
1. 一様分布（0〜1）
2. 正規分布（平均0、標準偏差1）
3. 指数分布（λ=1）

#### 要件

**入力**:
- サンプルサイズ: `[50, 100, 500, 1000]`
- 各サンプルサイズで3つの分布

**出力データフレーム**:
```
sample_size  distribution  mean      std       skewness  kurtosis
50           uniform       0.502     0.289     -0.05     -1.21
50           normal        0.015     0.995      0.03     -0.15
50           exponential   0.985     1.020      1.95      4.12
...
```

**使用するパターン**:
- パターン4: データフレームの各行（サンプルサイズと分布の組み合わせ）に対して統計量を計算

---

#### ヒント

1. **歪度（Skewness）と尖度（Kurtosis）**:
   - Python: `scipy.stats.skew()`, `scipy.stats.kurtosis()`
   - R: `moments`パッケージの`skewness()`, `kurtosis()`

2. **分布の生成**:
   - 一様分布: `np.random.uniform()` / `runif()`
   - 正規分布: `np.random.normal()` / `rnorm()`
   - 指数分布: `np.random.exponential()` / `rexp()`

3. **パラメータグリッドの作成**:
   - 全ての組み合わせを作る
   - Python: `itertools.product()`やネストしたリスト内包表記
   - R: `expand.grid()`

---

#### 解答例（Python）

**ファイル名**: `exercise02_data_pipeline.py`

```python
# 複数の統計分布の比較
import numpy as np
import pandas as pd
from scipy import stats
import itertools

# 指定した分布からサンプルを生成し、統計量を計算
def analyze_distribution(sample_size, distribution):
    """
    指定した分布からサンプルを生成し、統計量を計算
    """
    # 分布に応じてデータを生成
    if distribution == 'uniform':
        data = np.random.uniform(0, 1, sample_size)
    elif distribution == 'normal':
        data = np.random.normal(0, 1, sample_size)
    elif distribution == 'exponential':
        data = np.random.exponential(1, sample_size)
    else:
        raise ValueError(f"Unknown distribution: {distribution}")
    
    # 統計量を計算
    return pd.Series({
        'sample_size': sample_size,
        'distribution': distribution,
        'mean': data.mean(),
        'std': data.std(ddof=1),
        'skewness': stats.skew(data),
        'kurtosis': stats.kurtosis(data)
    })

# パラメータグリッドを作成
sample_sizes = [50, 100, 500, 1000]
distributions = ['uniform', 'normal', 'exponential']

# 全ての組み合わせ
param_grid = pd.DataFrame(
    list(itertools.product(sample_sizes, distributions)),
    columns=['sample_size', 'distribution']
)

print("統計分布の比較分析")
print("=" * 70)
print(f"サンプルサイズ: {sample_sizes}")
print(f"分布: {distributions}\n")

# 各組み合わせで分析
results = param_grid.apply(
    lambda row: analyze_distribution(row['sample_size'], row['distribution']),
    axis=1
)

print("結果:")
print("=" * 70)
print(results.to_string(index=False))

print("\n" + "=" * 70)
print("各分布の特徴:")
print("=" * 70)

# 分布ごとの平均的な特性
summary = results.groupby('distribution').agg({
    'mean': 'mean',
    'std': 'mean',
    'skewness': 'mean',
    'kurtosis': 'mean'
}).round(3)

print(summary)

print("\n解釈:")
print("- 一様分布: 歪度≈0, 尖度は負（平坦）")
print("- 正規分布: 歪度≈0, 尖度≈0（基準）")
print("- 指数分布: 歪度>0（右に歪む）, 尖度>0（裾が重い）")
```

**実行方法**:
```bash
$ cd ~/work
$ touch exercise02_data_pipeline.py
# VS Codeで上記コードを入力
$ python exercise02_data_pipeline.py
```

---

#### 解答例（R）

**ファイル名**: `exercise02_data_pipeline.R`

```r
# 複数の統計分布の比較
library(tidyverse)
library(moments)  # skewness, kurtosisのため

# 指定した分布からサンプルを生成し、統計量を計算
analyze_distribution <- function(sample_size, distribution) {
  # 分布に応じてデータを生成
  data <- switch(distribution,
    uniform = runif(sample_size, min = 0, max = 1),
    normal = rnorm(sample_size, mean = 0, sd = 1),
    exponential = rexp(sample_size, rate = 1),
    stop(paste("Unknown distribution:", distribution))
  )
  
  # 統計量を計算
  list(
    sample_size = sample_size,
    distribution = distribution,
    mean = mean(data),
    std = sd(data),
    skewness = skewness(data),
    kurtosis = kurtosis(data) - 3  # Excessを計算（Pythonと合わせる）
  )
}

# パラメータグリッドを作成
sample_sizes <- c(50, 100, 500, 1000)
distributions <- c('uniform', 'normal', 'exponential')

param_grid <- expand.grid(
  sample_size = sample_sizes,
  distribution = distributions,
  stringsAsFactors = FALSE
)

cat("統計分布の比較分析\n")
cat(strrep("=", 70), "\n")
cat("サンプルサイズ:", paste(sample_sizes, collapse = ", "), "\n")
cat("分布:", paste(distributions, collapse = ", "), "\n\n")

# 各組み合わせで分析
results <- param_grid %>%
  pmap_dfr(analyze_distribution)

cat("結果:\n")
cat(strrep("=", 70), "\n")
print(results, n = Inf)

cat("\n", strrep("=", 70), "\n")
cat("各分布の特徴:\n")
cat(strrep("=", 70), "\n")

# 分布ごとの平均的な特性
summary <- results %>%
  group_by(distribution) %>%
  summarise(
    mean = mean(mean),
    std = mean(std),
    skewness = mean(skewness),
    kurtosis = mean(kurtosis)
  ) %>%
  mutate(across(where(is.numeric), ~round(.x, 3)))

print(summary)

cat("\n解釈:\n")
cat("- 一様分布: 歪度≈0, 尖度は負（平坦）\n")
cat("- 正規分布: 歪度≈0, 尖度≈0（基準）\n")
cat("- 指数分布: 歪度>0（右に歪む）, 尖度>0（裾が重い）\n")
```

**実行方法**:
```bash
$ cd ~/work
$ touch exercise02_data_pipeline.R
# Rでmomentsパッケージをインストール（初回のみ）
$ R -e "install.packages('moments', repos='https://cran.r-project.org')"
# VS Codeで上記コードを入力
$ Rscript exercise02_data_pipeline.R
```

---

### 3.7.6.4 発展課題（オプション）

#### 課題1: 並列処理版の実装

演習課題1または2を並列処理で実装してみましょう。

**ヒント**:
- R: `future_map_dfr()`, `future_pmap_dfr()`
- Python: `parallel_apply()`

**比較すべき点**:
- 実行時間の短縮
- コードの変更点
- オーバーヘッドの影響

---

#### 課題2: 可視化の追加

結果を可視化してみましょう。

**Python**: `matplotlib`, `seaborn`  
**R**: `ggplot2`

**提案する可視化**:
- モンテカルロ法: サンプルサイズ vs 誤差の散布図
- 分布比較: 各分布のヒストグラムと統計量

---

#### 課題3: カスタムシミュレーション

自分でシミュレーション課題を設計してみましょう。

**アイデア例**:
- サイコロを振って特定の目が出るまでの回数
- ランダムウォーク（酔歩問題）
- 待ち行列シミュレーション

---

### 3.7.6.5 GitHub Copilotの活用

演習課題に取り組む際、以下のようにCopilotを活用してみましょう：

#### 効果的なプロンプト例

```
モンテカルロ法で円周率を推定するPython関数を作ってください。
n個のランダムな点を-1から1の範囲で生成し、
原点からの距離が1以下の点の割合を使ってπを推定します。
```

```
データフレームの各行（sample_sizeとdistribution）に対して、
指定した分布からデータを生成し、統計量を計算する関数を作ってください。
applyとlambdaを使ってください。
```

#### 学習のポイント

1. **段階的に実装**: まず1つの関数を完成させる
2. **動作確認**: 小さなデータでテスト
3. **拡張**: 全体のパイプラインに組み込む
4. **最適化**: 並列処理や可視化を追加

---

## 3.7.7 まとめと自己チェックリスト

### 3.7.7.1 本節の振り返り

本節では、**反復処理の4つのパターン**を学びました。

#### 学習した内容

**パターン1: 指定した回数→1次元データ**
- R: `replicate(n, 式)`
- Python: `[式 for i in range(n)]`
- 用途: シミュレーションの繰り返し

**パターン2: 1次元データ→1次元データ**
- R: `入力 %>% map_dbl(関数)`
- Python: `入力.apply(関数)`
- 用途: 各要素に関数を適用

**パターン3: 1次元データ→データフレーム**
- R: `入力 %>% map_dfr(関数)`
- Python: `入力.apply(関数)` (関数はSeriesを返す)
- 用途: 複数の統計量を計算

**パターン4: データフレーム→データフレーム**
- R: `入力 %>% pmap_dfr(関数)`
- Python: `入力.apply(lambda row: 関数(*row), axis=1)`
- 用途: パラメータグリッドで実験

**並列処理**
- R: `future_map_dbl()`, `future_map_dfr()`, `future_pmap_dfr()`
- Python: `parallel_apply()`
- 用途: 大規模計算の高速化

---

### 3.7.7.2 4つのパターンの使い分け

```
┌─────────────────────────────────────────────────┐
│ どのパターンを使うべきか？                        │
└─────────────────────────────────────────────────┘

Q1: 同じ処理を繰り返すだけ？
YES → パターン1 (replicate / 内包表記)
NO  → Q2へ

Q2: 入力はベクトル/リスト？
YES → Q3へ
NO  → パターン4 (pmap / apply with axis=1)

Q3: 出力は1つの値？
YES → パターン2 (map_dbl / apply)
NO  → パターン3 (map_dfr / apply with Series)
```

---

### 3.7.7.3 よくある間違いと対処法

#### 1. rep()や* 3で繰り返す

**❌ 間違い**: 最初の計算結果が複製されるだけ
```python
result = calculate()
[result] * 3  # 同じ値が3つ
```

**✅ 正しい**: パターン1を使う
```python
[calculate() for i in range(3)]  # 毎回新しい計算
```

---

#### 2. ベクトル化できるのにmapを使う

**❌ 非効率**: mapを使う必要がない
```r
c(4, 9, 16) %>% map_dbl(sqrt)
```

**✅ 効率的**: ベクトル化
```r
sqrt(c(4, 9, 16))
```

---

#### 3. lambda式の構文エラー

**❌ 間違い**: 列名を間違える
```python
df.apply(lambda row: func(row['wrong_name']), axis=1)
```

**✅ 正しい**: 列名を確認
```python
print(df.columns)  # 列名を確認
df.apply(lambda row: func(row['correct_name']), axis=1)
```

---

#### 4. 並列処理の不適切な使用

**❌ 非効率**: 小さなタスクで並列化
```python
pd.Series([1, 2, 3]).parallel_apply(lambda x: x * 2)
# オーバーヘッド > 高速化効果
```

**✅ 適切**: 大きなタスクで並列化
```python
pd.Series([10000] * 100).parallel_apply(heavy_calculation)
```

---

### 3.7.7.4 次のステップ

本節で学んだ反復処理は、以下の章でさらに活用します：

- **4章（確率と統計）**: シミュレーション、統計的推定
- **5章（データ可視化）**: 複数のグラフ生成
- **6章（機械学習）**: ハイパーパラメータ探索

---

### 3.7.7.5 自己チェックリスト

本節の内容を理解できたか、以下のチェックリストで確認しましょう。

#### A. 環境構築と基本操作

- [ ] WSL2のUbuntu環境で作業ディレクトリ(`~/work`)に移動できる
- [ ] `venvc`コマンドで仮想環境を起動できる
- [ ] プロンプトに`(class)`が表示されることを確認した
- [ ] VS Codeでターミナルを開くことができる
- [ ] Pythonファイル(`.py`)を作成できる
- [ ] Rファイル(`.R`)を作成できる
- [ ] `python ファイル名.py`でPythonプログラムを実行できる
- [ ] `Rscript ファイル名.R`でRプログラムを実行できる

#### B. 反復処理の概念理解

- [ ] 反復処理とは何か説明できる
- [ ] for文を使わない利点を3つ以上説明できる
- [ ] 4つのパターンの違いを説明できる
- [ ] 各パターンがどのような場面で使われるか理解している
- [ ] 直列処理と並列処理の違いを理解している
- [ ] データ並列の概念を理解している

#### C. パターン1: 指定した回数→1次元データ

- [ ] `replicate()`関数(R)の基本構文を理解している
- [ ] リスト内包表記(Python)の基本構文を理解している
- [ ] `replicate(n, 式)`で式が毎回評価されることを理解している
- [ ] `[式 for i in range(n)]`でループ変数iの役割を理解している
- [ ] `rep()`や`* 3`との違いを説明できる
- [ ] 同じ処理を複数回実行するサンプルプログラムを書けた
- [ ] 乱数を使った繰り返し計算を実装できる
- [ ] sample01_replicate.py/Rを実行して動作を確認した
- [ ] 繰り返し回数を変更して実験できる

#### D. パターン2: 1次元データ→1次元データ

- [ ] `map_dbl()`関数(R)の基本構文を理解している
- [ ] パイプ演算子`%>%`と組み合わせた使い方を理解している
- [ ] `apply()`メソッド(Python)の基本構文を理解している
- [ ] リスト内包表記とapplyの使い分けを理解している
- [ ] ベクトル化との違いを説明できる
- [ ] ベクトル化できる場合はそちらを優先すべきことを理解している
- [ ] 複数のサンプルサイズで計算を実行できる
- [ ] sample02_map_vector.py/Rを実行して動作を確認した
- [ ] 関数を変更して異なる計算を試せる
- [ ] パターン1とパターン2の関係を理解している

#### E. パターン3: 1次元データ→データフレーム

- [ ] `map_dfr()`関数(R)の基本構文を理解している
- [ ] 関数がリストを返す必要があることを理解している
- [ ] `apply()`(Python)で関数がSeriesを返す必要があることを理解している
- [ ] `pd.Series()`でindexパラメータを指定できる
- [ ] 複数の統計量を同時に計算できる
- [ ] 平均と標準偏差の意味を理解している
- [ ] 結果がデータフレームになることを確認した
- [ ] sample03_map_dataframe.py/Rを実行して動作を確認した
- [ ] 別の統計量(最小値、最大値など)を追加できる
- [ ] サンプルサイズと統計量の関係を解釈できる

#### F. パターン4: データフレーム→データフレーム

- [ ] `pmap_dfr()`関数(R)の基本構文を理解している
- [ ] データフレームの列が関数の引数に対応することを理解している
- [ ] `apply(axis=1)`(Python)の意味を理解している
- [ ] lambda式の基本構文を理解している
- [ ] `lambda row: 関数(row['x'], row['y'])`の書き方を理解している
- [ ] `*row`によるアンパックの意味を理解している
- [ ] パラメータグリッドの概念を理解している
- [ ] 複数引数を持つ関数を各行に適用できる
- [ ] sample04_pmap.py/Rを実行して動作を確認した
- [ ] パラメータの組み合わせを変更して実験できる
- [ ] 結果を解釈して傾向を読み取れる

#### G. 並列処理

- [ ] 並列処理の概念を理解している
- [ ] CPUコアの役割を理解している
- [ ] 並列処理の利点を3つ以上説明できる
- [ ] 並列処理の注意点を3つ以上説明できる
- [ ] 独立性の重要性を理解している
- [ ] オーバーヘッドの概念を理解している
- [ ] furrr パッケージ(R)の基本的な使い方を理解している
- [ ] `plan(multisession)`の意味を理解している
- [ ] `future_map_dbl()`などの関数名規則を理解している
- [ ] `.options = furrr_options(seed = TRUE)`の役割を理解している
- [ ] pandarallel パッケージ(Python)の基本的な使い方を理解している
- [ ] `pandarallel.initialize()`の役割を理解している
- [ ] `parallel_apply()`の使い方を理解している
- [ ] sample05_parallel.py/Rを実行して動作を確認した
- [ ] 通常の処理と並列処理の実行時間を比較した
- [ ] タスクの大きさと高速化効果の関係を理解している
- [ ] いつ並列処理を使うべきか判断できる

#### H. 統合演習

- [ ] exercise01_monte_carlo.py/Rのいずれかを完成させた
- [ ] モンテカルロ法の原理を理解している
- [ ] サンプルサイズと推定精度の関係を確認した
- [ ] exercise02_data_pipeline.py/Rのいずれかを完成させた
- [ ] 複数の統計分布の特性を比較できた
- [ ] 歪度と尖度の意味を理解している
- [ ] パラメータグリッドを作成できる
- [ ] 複数のパターンを組み合わせて使える

#### I. GitHub Copilot活用スキル

- [ ] Copilot Chatを開くことができる
- [ ] 具体的なプロンプトを書くことができる
- [ ] 生成されたコードを読んで理解できる
- [ ] コードの意味がわからない部分をCopilotに質問できる
- [ ] 生成されたコードを実行して動作確認できる
- [ ] エラーが出たときに原因を調べられる
- [ ] コメントを先に書いてCopilotに提案させることができる
- [ ] 段階的にコードを書くアプローチを実践できる
- [ ] 生成されたコードを少しずつ変更して実験できる
- [ ] Copilotに頼りすぎず、自分で考えることができる

#### J. プログラミングスキル

- [ ] 関数を定義できる(R: `function()`, Python: `def`)
- [ ] 乱数を生成できる(R: `runif()`, Python: `np.random.random()`)
- [ ] 平均を計算できる(R: `mean()`, Python: `np.mean()`)
- [ ] 標準偏差を計算できる(R: `sd()`, Python: `np.std(ddof=1)`)
- [ ] データフレームを作成できる
- [ ] 列を指定してデータにアクセスできる
- [ ] 実行時間を計測できる
- [ ] エラーメッセージを読んで原因を推測できる
- [ ] デバッグのために`print()`文を挿入できる
- [ ] コードにコメントを適切に付けられる

#### K. データサイエンススキル

- [ ] シミュレーションの目的を理解している
- [ ] サンプルサイズの重要性を理解している
- [ ] 大数の法則の概念を理解している
- [ ] 期待値と実測値の違いを理解している
- [ ] 統計量(平均、標準偏差など)の意味を理解している
- [ ] 結果を解釈して傾向を読み取れる
- [ ] 理論値と実験値を比較できる
- [ ] 統計的な推論の基礎を理解している

---

### 3.7.7.6 達成度評価

チェックした項目数に応じて、達成度を確認しましょう。

**全75項目中**:

- **65項目以上 (87%以上)**: 🎉 **優秀！** 本節の内容を十分に理解しています
- **55-64項目 (73-85%)**: 👍 **良好！** 基本は理解できています。不明な点を復習しましょう
- **45-54項目 (60-72%)**: 📖 **要復習** いくつかの重要な概念が不足しています
- **44項目以下 (59%以下)**: ⚠️ **再学習推奨** 本節を最初から復習することを推奨します

---

### 3.7.7.7 さらに学ぶために

#### 推奨する学習リソース

**公式ドキュメント**:
- R tidyverse: https://www.tidyverse.org/
- pandas: https://pandas.pydata.org/docs/
- NumPy: https://numpy.org/doc/

**発展的なトピック**:
- 関数型プログラミング
- MapReduceパラダイム
- 分散コンピューティング
- GPUを使った並列計算

**次に読むべき章**:
- 4章: 確率と統計（シミュレーションの応用）
- 5章: データ可視化（結果の図示）

---

### 3.7.7.8 最後に

反復処理は、データサイエンスの基礎的かつ重要なスキルです。本節で学んだ4つのパターンは、実務でも頻繁に使われます。

**重要なポイント**:
1. **適切なパターンを選ぶ**: 入力と出力の型で判断
2. **シンプルに保つ**: ベクトル化できるなら、それを優先
3. **並列化は慎重に**: 効果があるときだけ使う
4. **AI と協働する**: Copilotを活用しながら理解を深める
